<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
 
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Zhuo Xu<sup>1</sup></a><sup>*</sup>,</span>
                <span class="author-block">
                  Xiang Xiang<sup>1,2</sup></a><sup>*</sup><sup>†</sup>,</span>
                  <span class="author-block">
                   Yifan Liang<sup>1</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> National Key Lab of Multi-Spectral Information Intelligent Processing Technology
School of Artificial Intelligence and Automation,Huazhong University of Science and Technology, China
                    
                    <span class="author-block"><sup>2</sup> Peng Cheng National Laboratory, Shenzhen, China

                    <br>CVPR 2025 (HighLight)</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution; co-first author</small></span>
                      <span class="eql-cntrb"><small><br><sup>†</sup>Correspondence to xex@hust.edu.cn; also with School of CST</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection_CVPR_2025_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/HAIV-Lab/OSPCoOp_Imagenet-bg" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://www.kaggle.com/datasets/xiangexiang/imagenet-bg-ospcoop-cvpr2025" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-book-open"></i>
                </span>
                <span>Dataset</span>
              </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/CVPR25.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
             </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language models (VLMs), such as CLIP, have shown remarkable capabilities in downstream tasks. However, the coupling of semantic information between the foreground and the background in images leads to significant shortcut issues that adversely affect out-of-distribution (OOD) detection abilities. When confronted with a background OOD sample, VLMs are prone to misidentifying it as in-distribution (ID) data. In this paper, we analyze the OOD problem from the perspective of shortcuts in VLMs and propose OSPCoOp which includes background decoupling and mask-guided region regularization. We first decouple images into ID-relevant and ID-irrelevant regions and utilize the latter to generate a large number of augmented OOD background samples as pseudo-OOD supervision. We then use the masks from background decoupling to adjust the model's attention, minimizing its focus on ID-irrelevant regions. To assess the model's robustness against background interference, we introduce a new OOD evaluation dataset, ImageNet-Bg, which solely consists of background images with all ID-relevant regions removed. Our method demonstrates exceptional performance in few-shot scenarios, achieving strong results even in one-shot setting, and outperforms existing methods.         </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- 新增的图片和说明文字部分 -->
<!-- Pipeline Section -->
<section class="has-text-centered" style="margin-top: 2rem;">
  <!-- Title -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">  <!-- Wider column -->
        <h2 class="title is-2 has-text-centered">Pipeline</h2>  <!-- Larger title -->
      </div>
    </div>
  </div>

  <!-- Image -->
  <figure class="image" style="width: 85%; max-width: 900px; min-width: 700px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/pipeline.png" alt="Method overview">
  </figure>

  <!-- Description -->
  <div class="container is-fluid">  <!-- 改为 is-fluid 让容器更宽 -->
    <div class="columns is-centered">
      <div class="column is-11">  <!-- 改为 is-11 (11/12宽度) -->
        <div class="content has-text-justified" style="font-size: 1.1rem; max-width: 1000px; margin: 0 auto;">
          Pipeline of OSPCoOp. We first decouple the images into foreground and background to provide pseudo-OOD supervision.
Subsequently, we generate ID and pseudo-OOD augmented samples and then use the masks obtained during the background decoupling
process to provide regional supervision for ID and OOD samples, guiding the model to ignore the ID-irrelevant regions.
        </div>
      </div>
    </div>
  </div>

<!-- Dataset Section -->
<section class="has-text-centered" style="margin-top: 3rem;">  <!-- More spacing -->
  <!-- Title -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered">Dataset ImageNet-Bg</h2>  <!-- Larger title -->
      </div>
    </div>
  </div>

  <!-- First Image -->
  <figure class="image" style="width: 85%; max-width: 900px; min-width: 700px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/imagenet-bg.png" alt="ImageNet-Bg overview">
  </figure>

  <!-- Second Image -->
  <figure class="image" style="width: 85%; max-width: 900px; min-width: 700px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/imagenet-bg-more.png" alt="ImageNet-Bg examples">
  </figure>
</section>

    <!-- Description -->
  <div class="container is-fluid">  <!-- 改为 is-fluid 让容器更宽 -->
    <div class="columns is-centered">
      <div class="column is-11">  <!-- 改为 is-11 (11/12宽度) -->
        <div class="content has-text-justified" style="font-size: 1.1rem; max-width: 1000px; margin: 0 auto;">
          To evaluate the robustness of the model against background interference, we propose an ImageNet background interference test set, ImageNet-Bg, based on the ImageNet validation set with 48,285 images. All images in this dataset are generated by removing ID-relevant regions from samples in the ImageNet validation set. We filter the images to obtain the ImageNet-Bg(S) test set, which contains purer background information with 24,863 images.

        </div>
      </div>
    </div>
  </div>

  <!-- Dataset Section -->
<section class="has-text-centered" style="margin-top: 3rem;">  <!-- More spacing -->
  <!-- Title -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered">Results on ImageNet and ImageNet-Bg</h2>  <!-- Larger title -->
      </div>
    </div>
  </div>

  <!-- First Image -->
  <figure class="image" style="width: 85%; max-width: 1000px; min-width: 800px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/results-all.png" alt="ImageNet-Bg overview">
  </figure>



    <!-- Description -->
  <div class="container is-fluid">  <!-- 改为 is-fluid 让容器更宽 -->
    <div class="columns is-centered">
      <div class="column is-11">  <!-- 改为 is-11 (11/12宽度) -->
        <div class="content has-text-justified" style="font-size: 1.1rem; max-width: 1000px; margin: 0 auto;">
          <p>(Left) Results on ImageNet-1K benchmark with iNaturalist, SUN, Places, and Texture datasets.  Our method achieves an average of 94.75% AUR and 25.13% FPR—a 0.91% AUR improvement and a 1.34% FPR reduction over the state-of-the-art few-shot training method. For OOD datasets with significant background interference (e.g., SUN and Places), our method shows notable gains, reaching 96.74% AUR on SUN and 94.01% AUR on Places.
          <p>(Right) Results on ImageNet-Bg. Results reveal that most methods underperform compared to the training-free CLIP baselines (MCM and GL-MCM). Besides, parameter-efficient fine-tuning demonstrates limited effectiveness in enhancing model robustness against background interference. Extensive parameter tuning may inadvertently reintroduce shortcut learning behaviors, ultimately diminishing the model's robustness against background attacks.
        </div>
      </div>
    </div>
  </div>



  <!-- Dataset Section -->
<section class="has-text-centered" style="margin-top: 3rem;">  <!-- More spacing -->
  <!-- Title -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered">Comparison Visualization</h2>  <!-- Larger title -->
      </div>
    </div>
  </div>

  <!-- First Image -->
  <figure class="image" style="width: 85%; max-width: 1000px; min-width: 800px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/ID.png" alt="ImageNet-Bg overview">
  </figure>

    <!-- Description -->
<div class="container is-fluid">
  <div class="columns is-centered">
    <div class="column is-11">
      <div class="content has-text-centered" style="font-size: 1.1rem; max-width: 1000px; margin: 0 auto;">
          Visualization on ImageNet-1K ID dataset, where gray regions indicate areas the model identifies as ID-relevant.
                  </div>
      </div>
    </div>
  </div>

  <!-- First Image -->
  <figure class="image" style="width: 85%; max-width: 1000px; min-width: 800px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/OOD-SUN.png" alt="ImageNet-Bg overview">
  </figure>
  <!-- First Image -->
  <figure class="image" style="width: 85%; max-width: 1000px; min-width: 800px; margin: 1.5rem auto;">  <!-- Larger image -->
    <img src="static/images/OOD-Bg.png" alt="ImageNet-Bg overview">
  </figure>
<!-- Description -->
<div class="container is-fluid">
  <div class="columns is-centered">
    <div class="column is-11">
      <div class="content has-text-centered" style="font-size: 1.1rem; max-width: 1000px; margin: 0 auto;">
        Visualization on SUN (top) and ImageNet-Bg (bottom) OOD dataset, where darker colors indicate higher similarity with ID categories.
      </div>
    </div>
  </div>
</div>

<!-- End image carousel -->

<!-- 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="text-align: left;">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{Xu_2025_CVPR,
  author    = {Xu, Zhuo and Xiang, Xiang and Liang, Yifan},
  title     = {Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection},
  booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},
  month     = {June},
  year      = {2025},
  pages     = {15402-15412}
}</code></pre>
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
             <br>We would like to thank Keunhong Park for sharing the template.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
